# Implement G-means algorithm                                      #
####################################################################
rm(list=ls())
library(ellipse)
library(mvnTest)
library(MASS)
#Set the directory where the data is present
setwd("~/Desktop/hwfds45/")
# X is the input dataset(it is a dataframe contain multiple cols reading from csv file)
Gmeans <- function(X,alpha = 0.0001,k=1){
#Adding the column in data to represent the cluster number
cluster_column <- ncol(X) + 1
# Initially we start with only 1 cluster, hence every row belongs to only 1 cluster
X[, cluster_column] <- rep(1, nrow(X))
#Setting the header name for the cluster column
names(X)[cluster_column] <- "cluster"
# Finding the first center as the initial number of clusters = 1
first_center <- kmeans(X[, -cluster_column], centers = k)
cluster_centers <- first_center$centers
#Counting the clusters
cluster = 1   #Initially we start with just 1 cluster
# Checking for every centre if it can be further divided into two centers
while(1)
{
centers <- nrow(cluster_centers)
#Variable indicating whether to stop or not
stop = 0
#Checking for every center if it correctly represents the data or do we need to split the center
for(i in 1:centers){
#Considering the centres of every dimension
c <- cluster_centers[i, ]
#Create the subset of data which belong to that cluster for the respective center
Z <- subset(X, cluster == i)
#W <- scan("Z[, -cluster_column]")
#Test the data for to detect if it follows a Gaussian distribution
#test <- ad.test(Z[, -cluster_column])
test <- AD.test(Z[, -cluster_column], qqplot = FALSE)
# Checking if the test value is less than the critical value for alpha = 0.00001
if((test@p.value < -alpha) | (test@p.value > alpha))
{
#Performing kmeans on Z to find two new centers for the current center
k <- kmeans(Z[, -cluster_column], centers = 2)
stop = 1
cdash <- k$centers    #New centers
new_cluster <- k$cluster    #New clusters
cluster = cluster + 1
cluster_centers[i, ] = cdash[1, ]
cluster_centers <- rbind(cluster_centers, cdash[2, ])  #Adding the new centers to the old center data
# Now we order the clusters
m = dist(rbind(X[which(new_clusters == 1)[1], -cluster_column], cdash[1, ]))
n = dist(rbind(X[which(newclusters == 1)[1], -cluster_column], cdash[2, ]))
if(m < n)
{
new_cluster[new_clusters == 2] = numclusters
new_clusters[new_clusters == 1] = i
}
else
{
new_cluster[new_clusters == 1] = numclusters
new_clusters[new_clusters == 2] = i
}
X[which(X$cluster == i), p] = new_clusters    #Putting the updated cluster number back to the original data
print("HELLO")
#Plotting the clusters
for(j in 1:(cluster_column - 2))
{
plot(X[, c(j, j + 1)], col = X$cluster, pch = '.', cex = 5)
for(m in 1:cluster)
{
l <- cov(X[which(X$cluster == m), c(j, j + 1)])
ellps <- ellipse(l, centre = centers[m, c(j, j + 1)])
lines(ellps, col = m, lwd = 3)
points(x = centers[m, j], y = centers[m, j + 1], pch = '*', cex = 2)
}
}
}
}
if(stop == 0)
{
break;
}
}
}
#Reading the data
data = read.table("hw45-r3b-test-data.csv",sep = ",",header = T)
#Calling the Gmeans function
Gmeans(data,0.0001,1)
library(bnlearn)
#Read the data
a = read.table("~/Desktop/hwfds45/bn-data.csv", header = TRUE, sep = "," )
bn_df <- data.frame(a[2:7])
res <- hc(bn_df)
plot(res, main = "Bayesian network of the sample data")
fittedbn <- bn.fit(res, data = bn_df)
fittedbn
####################################################################
# Implement G-means algorithm                                      #
####################################################################
rm(list=ls())
library(ellipse)
library(mvnTest)
library(MASS)
#Set the directory where the data is present
setwd("~/Desktop/hwfds45/")
# X is the input dataset(it is a dataframe contain multiple cols reading from csv file)
Gmeans <- function(X,alpha = 0.0001,k=1){
#Adding the column in data to represent the cluster number
cluster_column <- ncol(X) + 1
# Initially we start with only 1 cluster, hence every row belongs to only 1 cluster
X[, cluster_column] <- rep(1, nrow(X))
#Setting the header name for the cluster column
names(X)[cluster_column] <- "cluster"
# Finding the first center as the initial number of clusters = 1
first_center <- kmeans(X[, -cluster_column], centers = k)
cluster_centers <- first_center$centers
#Counting the clusters
cluster = 1   #Initially we start with just 1 cluster
# Checking for every centre if it can be further divided into two centers
while(1)
{
centers <- nrow(cluster_centers)
#Variable indicating whether to stop or not
stop = 0
#Checking for every center if it correctly represents the data or do we need to split the center
for(i in 1:centers){
#Considering the centres of every dimension
c <- cluster_centers[i, ]
#Create the subset of data which belong to that cluster for the respective center
Z <- subset(X, cluster == i)
#W <- scan("Z[, -cluster_column]")
#Test the data for to detect if it follows a Gaussian distribution
#test <- ad.test(Z[, -cluster_column])
test <- AD.test(Z[, -cluster_column], qqplot = FALSE)
# Checking if the test value is less than the critical value for alpha = 0.00001
if((test@p.value < -alpha) | (test@p.value > alpha))
{
#Performing kmeans on Z to find two new centers for the current center
k <- kmeans(Z[, -cluster_column], centers = 2)
stop = 1
cdash <- k$centers    #New centers
new_cluster <- k$cluster    #New clusters
cluster = cluster + 1
cluster_centers[i, ] = cdash[1, ]
cluster_centers <- rbind(cluster_centers, cdash[2, ])  #Adding the new centers to the old center data
# Now we order the clusters
m = dist(rbind(X[which(new_clusters == 1)[1], -cluster_column], cdash[1, ]))
n = dist(rbind(X[which(newclusters == 1)[1], -cluster_column], cdash[2, ]))
if(m < n)
{
new_cluster[new_clusters == 2] = numclusters
new_clusters[new_clusters == 1] = i
}
else
{
new_cluster[new_clusters == 1] = numclusters
new_clusters[new_clusters == 2] = i
}
X[which(X$cluster == i), p] = new_clusters    #Putting the updated cluster number back to the original data
print("HELLO")
#Plotting the clusters
for(j in 1:(cluster_column - 2))
{
plot(X[, c(j, j + 1)], col = X$cluster, pch = '.', cex = 5)
for(m in 1:cluster)
{
l <- cov(X[which(X$cluster == m), c(j, j + 1)])
ellps <- ellipse(l, centre = centers[m, c(j, j + 1)])
lines(ellps, col = m, lwd = 3)
points(x = centers[m, j], y = centers[m, j + 1], pch = '*', cex = 2)
}
}
}
}
if(stop == 0)
{
break;
}
}
}
#Reading the data
data = read.table("hw45-r3b-test-data.csv",sep = ",",header = T)
#Calling the Gmeans function
Gmeans(data,0.0001,1)
####################################################################
# Implement G-means algorithm                                      #
####################################################################
rm(list=ls())
library(ellipse)
library(mvnTest)
library(MASS)
#Set the directory where the data is present
setwd("~/Desktop/hwfds45/")
# X is the input dataset(it is a dataframe contain multiple cols reading from csv file)
Gmeans <- function(X,alpha = 0.0001,k=1){
#Adding the column in data to represent the cluster number
cluster_column <- ncol(X) + 1
# Initially we start with only 1 cluster, hence every row belongs to only 1 cluster
X[, cluster_column] <- rep(1, nrow(X))
#Setting the header name for the cluster column
names(X)[cluster_column] <- "cluster"
# Finding the first center as the initial number of clusters = 1
first_center <- kmeans(X[, -cluster_column], centers = k)
cluster_centers <- first_center$centers
#Counting the clusters
cluster = 1   #Initially we start with just 1 cluster
# Checking for every centre if it can be further divided into two centers
while(1)
{
centers <- nrow(cluster_centers)
#Variable indicating whether to stop or not
stop = 0
#Checking for every center if it correctly represents the data or do we need to split the center
for(i in 1:centers){
#Considering the centres of every dimension
c <- cluster_centers[i, ]
#Create the subset of data which belong to that cluster for the respective center
Z <- subset(X, cluster == i)
#Test the data for to detect if it follows a Gaussian distribution
#test <- ad.test(Z[, -cluster_column])
test <- AD.test(Z[, -cluster_column], qqplot = FALSE)
# Checking if the test value is less than the critical value for alpha = 0.00001
if((test@p.value < -alpha) | (test@p.value > alpha))
{
#Performing kmeans on Z to find two new centers for the current center
k <- kmeans(Z[, -cluster_column], centers = 2)
stop = 1
cdash <- k$centers    #New centers
new_cluster <- k$cluster    #New clusters
cluster = cluster + 1
cluster_centers[i, ] = cdash[1, ]
cluster_centers <- rbind(cluster_centers, cdash[2, ])  #Adding the new centers to the old center data
# Now we order the clusters
m = dist(rbind(X[which(new_clusters == 1)[1], -cluster_column], cdash[1, ]))
n = dist(rbind(X[which(new_clusters == 1)[1], -cluster_column], cdash[2, ]))
if(m < n)
{
new_cluster[new_clusters == 2] = numclusters
new_clusters[new_clusters == 1] = i
}
else
{
new_cluster[new_clusters == 1] = numclusters
new_clusters[new_clusters == 2] = i
}
X[which(X$cluster == i), p] = new_clusters    #Putting the updated cluster number back to the original data
print("HELLO")
#Plotting the clusters
for(j in 1:(cluster_column - 2))
{
plot(X[, c(j, j + 1)], col = X$cluster, pch = '.', cex = 5)
for(m in 1:cluster)
{
l <- cov(X[which(X$cluster == m), c(j, j + 1)])
ellps <- ellipse(l, centre = centers[m, c(j, j + 1)])
lines(ellps, col = m, lwd = 3)
points(x = centers[m, j], y = centers[m, j + 1], pch = '*', cex = 2)
}
}
}
}
if(stop == 0)
{
break;
}
}
}
#Reading the data
data = read.table("hw45-r3b-test-data.csv",sep = ",",header = T)
#Calling the Gmeans function
Gmeans(data,0.0001,1)
# importing the library 'ISLR'
library(ISLR)
# Summary of the default dataset which contains the columns
# of credit card default, student, balance and income
summary(Default)
# Setting a random seed
set.seed(5)
# Fitting the logistic model to the default dataset
glm_fit = glm(default ~ balance + income, data = Default, family = binomial)
glm_fit
validset = function(){
# Now validation set approach is performed to calculate test error
# Step 1: Dividing the dataset into training set (75% of the actual dataset) and validation set(25% of the actual dataset)
sets <- sample(2, dim(Default)[1]/2, replace = TRUE, prob = c(0.50, 0.50))
training_data <- Default[sets == 1,]
validation_data <- Default[sets == 2,]
training_fit <- glm(default ~ balance + income, training_data, family = binomial)
probs = predict(training_fit, validation_data)
probs
pred = rep("No", dim(validation_data)[1])
pred[probs > 0.5] = "Yes"
return(mean(pred != validation_data$default))
}
validset()
validset()
validset()
validset()
# Now validation set approach is performed to calculate test error
# Step 1: Dividing the dataset into training set (75% of the actual dataset) and validation set(25% of the actual dataset)
sets <- sample(2, dim(Default)[1]/2, replace = TRUE, prob = c(0.50, 0.50))
training_data <- Default[sets == 1,]
validation_data <- Default[sets == 2,]
training_fit <- glm(default ~ balance + income + student, training_data, family = binomial)
probs = predict(training_fit, validation_data)
probs
pred = rep("No", dim(validation_data)[1])
pred[probs > 0.5] = "Yes"
return(mean(pred != validation_data$default))
# importing the library 'ISLR'
library(ISLR)
# Summary of the default dataset which contains the columns
# of credit card default, student, balance and income
summary(Default)
# Setting a random seed
set.seed(5)
# Fitting the logistic model to the default dataset
glm_fit = glm(default ~ balance + income, data = Default, family = binomial)
glm_fit
validset = function(){
# Now validation set approach is performed to calculate test error
# Step 1: Dividing the dataset into training set (75% of the actual dataset) and validation set(25% of the actual dataset)
sets <- sample(2, dim(Default)[1]/2, replace = TRUE, prob = c(0.50, 0.50))
training_data <- Default[sets == 1,]
validation_data <- Default[sets == 2,]
training_fit <- glm(default ~ balance + income, training_data, family = binomial)
probs = predict(training_fit, validation_data)
probs
pred = rep("No", dim(validation_data)[1])
pred[probs > 0.5] = "Yes"
return(mean(pred != validation_data$default))
}
validset()
validset()
validset()
validset()
# Now validation set approach is performed to calculate test error
# Step 1: Dividing the dataset into training set (75% of the actual dataset) and validation set(25% of the actual dataset)
sets <- sample(2, dim(Default)[1]/2, replace = TRUE, prob = c(0.50, 0.50))
training_data <- Default[sets == 1,]
validation_data <- Default[sets == 2,]
training_fit <- glm(default ~ balance + income + student, training_data, family = binomial)
probs = predict(training_fit, validation_data)
probs
pred = rep("No", dim(validation_data)[1])
pred[probs > 0.5] = "Yes"
(mean(pred != validation_data$default))
# importing the library 'ISLR'
library(ISLR)
# Summary of the default dataset which contains the columns
# of credit card default, student, balance and income
summary(Default)
# Setting a random seed
set.seed(5)
# Fitting the logistic model to the default dataset
glm_fit = glm(default ~ balance + income, data = Default, family = binomial)
glm_fit
validset = function(){
# Now validation set approach is performed to calculate test error
# Step 1: Dividing the dataset into training set (75% of the actual dataset) and validation set(25% of the actual dataset)
sets <- sample(2, dim(Default)[1]/2, replace = TRUE, prob = c(0.50, 0.50))
training_data <- Default[sets == 1,]
validation_data <- Default[sets == 2,]
training_fit <- glm(default ~ balance + income, training_data, family = binomial)
probs = predict(training_fit, validation_data)
pred = rep("No", dim(validation_data)[1])
pred[probs > 0.5] = "Yes"
return(mean(pred != validation_data$default))
}
validset()
validset()
validset()
validset()
# Now validation set approach is performed to calculate test error
# Step 1: Dividing the dataset into training set (75% of the actual dataset) and validation set(25% of the actual dataset)
sets <- sample(2, dim(Default)[1]/2, replace = TRUE, prob = c(0.50, 0.50))
training_data <- Default[sets == 1,]
validation_data <- Default[sets == 2,]
training_fit <- glm(default ~ balance + income + student, training_data, family = binomial)
probs = predict(training_fit, validation_data)
pred = rep("No", dim(validation_data)[1])
pred[probs > 0.5] = "Yes"
(mean(pred != validation_data$default))
# importing the library 'ISLR'
library(ISLR)
# Summary of the default dataset which contains the columns
# of credit card default, student, balance and income
summary(Default)
# Setting a random seed
set.seed(5)
# Fitting the logistic model to the default dataset
glm_fit = glm(default ~ balance + income, data = Default, family = binomial)
glm_fit
validset = function(){
# Now validation set approach is performed to calculate test error
# Step 1: Dividing the dataset into training set (75% of the actual dataset) and validation set(25% of the actual dataset)
sets <- sample(2, dim(Default)[1]/2, replace = TRUE, prob = c(0.50, 0.50))
training_data <- Default[sets == 1,]
validation_data <- Default[sets == 2,]
training_fit <- glm(default ~ balance + income, training_data, family = binomial)
probs = predict(training_fit, validation_data)
pred = rep("No", dim(validation_data)[1])
pred[probs > 0.5] = "Yes"
return(mean(pred != validation_data$default))
}
validset()
validset()
validset()
validset()
# Now validation set approach is performed to calculate test error including the dummy variable "student"
# Step 1: Dividing the dataset into training set (75% of the actual dataset) and validation set(25% of the actual dataset)
sets <- sample(2, dim(Default)[1]/2, replace = TRUE, prob = c(0.50, 0.50))
training_data <- Default[sets == 1,]
validation_data <- Default[sets == 2,]
training_fit <- glm(default ~ balance + income + student, training_data, family = binomial)
probs = predict(training_fit, validation_data)
pred = rep("No", dim(validation_data)[1])
pred[probs > 0.5] = "Yes"
(mean(pred != validation_data$default))
rm(list=ls())
#Using the pixmap library to read the images
library(pixmap)
#Setting the working directory
setwd("~/Desktop/hwfds45/faces-corrected")
#x <- read.pnm(file = "subject01.centerlight.pgm")
#Specifying the pattern of images to be read
list.files(pattern = ".pgm$")
l.files <- list.files(pattern = ".pgm$")
#Initialising variable to store the images
l.data <- list()
pdata <- list()
mat = matrix(nrow = 45045, ncol =0)
#Reading the images from the folder
for (i in 1:length(l.files))
{
l.data[i] <- read.pnm(l.files[i])
}
# Converting the images to vectors and adding the vectors to a matrix
for (i in 1:length(l.files))
{
xy <- getChannels(l.data[[i]])
dim(xy) <- c(dim(xy)[1]*dim(xy)[2], 1)
xy = xy - mean(xy)     # Centralizing every vector representing an image
mat = cbind(mat, xy)
}
# Calculating the covariance matrix
Cov = t(mat) %*% mat
# Computing Eigen vectors of the covariance matrix
par = eigen(Cov)$vectors
# Projecting the data to get the eigennfaces
final = mat %*% par
efaces <- list()
# Extracting the eigen faces and saving it on disk
ab = matrix(nrow = 45045, ncol = 0)
#Setting the directory to save the generated eigenfaces
setwd("~/Desktop/hwfds45/faces-generated")
rotate <- function(x) t(apply(x, 2, rev))
for (i in 1:length(l.files))
{
ab = matrix(final[,i], nrow = 45045, ncol = 1)
dim(ab) <- c(231, 195)
rotate(ab)
cd <- ab
z <- pixmapGrey(cd)
write.pnm(z,file = l.files[i])
}
rm(list=ls())
#Using the pixmap library to read the images
library(pixmap)
#Setting the working directory
setwd("~/Desktop/hwfds45/faces-corrected")
#x <- read.pnm(file = "subject01.centerlight.pgm")
#Specifying the pattern of images to be read
list.files(pattern = ".pgm$")
l.files <- list.files(pattern = ".pgm$")
#Initialising variable to store the images
l.data <- list()
pdata <- list()
mat = matrix(nrow = 45045, ncol =0)
#Reading the images from the folder
for (i in 1:length(l.files))
{
l.data[i] <- read.pnm(l.files[i])
}
# Converting the images to vectors and adding the vectors to a matrix
for (i in 1:length(l.files))
{
xy <- getChannels(l.data[[i]])
dim(xy) <- c(dim(xy)[1]*dim(xy)[2], 1)
xy = xy - mean(xy)     # Centralizing every vector representing an image
mat = cbind(mat, xy)
}
# Calculating the covariance matrix
Cov = t(mat) %*% mat
# Computing Eigen vectors of the covariance matrix
par = eigen(Cov)$vectors
# Projecting the data to get the eigennfaces
final = mat %*% par
efaces <- list()
# Extracting the eigen faces and saving it on disk
ab = matrix(nrow = 45045, ncol = 0)
#Setting the directory to save the generated eigenfaces
setwd("~/Desktop/hwfds45/faces-generated")
rotate <- function(x) t(apply(x, 2, rev))
for (i in 1:length(l.files))
{
ab = matrix(final[,i], nrow = 45045, ncol = 1)
dim(ab) <- c(231, 195)
rotate(ab)
cd <- ab
z <- pixmapGrey(cd)
write.pnm(z,file = l.files[i])
}
